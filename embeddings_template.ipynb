{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embeddings_template.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "6isEl_M1lSIV",
        "JO0s-T8clSIq"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/griu/deeplearningupc/blob/master/embeddings_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oyW1JgAWlSG3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings and Data Processing Lab\n",
        "\n",
        "In this lab we explore how to work with word embeddings (training, usage and visualization) and check how embeddings differ according to the data processing we apply. We use input corpora as words or stems or lemmas and we learn how to obtain the part of speech of a word. \n"
      ]
    },
    {
      "metadata": {
        "id": "fWPHexI1lSG5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Packages\n",
        "\n",
        "We will use `gensim` for word embeddings and `nltk` for some data processing tasks. Also a logger, `matplotlib` for visualisation and `sklearn` for representations."
      ]
    },
    {
      "metadata": {
        "id": "CmlkEZY9lSG6",
        "colab_type": "code",
        "colab": {},
        "outputId": "d44723b8-defc-4a43-83a1-5db4aea8f377"
      },
      "cell_type": "code",
      "source": [
        "!pip install logging"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting logging\n",
            "  Downloading https://files.pythonhosted.org/packages/93/4b/979db9e44be09f71e85c9c8cfc42f258adfb7d93ce01deed2788b2948919/logging-0.4.9.6.tar.gz (96kB)\n",
            "Building wheels for collected packages: logging\n",
            "  Running setup.py bdist_wheel for logging: started\n",
            "  Running setup.py bdist_wheel for logging: finished with status 'done'\n",
            "  Stored in directory: C:\\Users\\ferrancm1\\AppData\\Local\\pip\\Cache\\wheels\\7d\\2e\\cb\\a51fbdf351b2efebcf857f8b2c8d59b6ccd44ea2e9bb4005d6\n",
            "Successfully built logging\n",
            "Installing collected packages: logging\n",
            "Successfully installed logging-0.4.9.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "distributed 1.21.8 requires msgpack, which is not installed.\n",
            "grin 1.2.1 requires argparse>=1.1, which is not installed.\n",
            "You are using pip version 10.0.1, however version 19.0.3 is available.\n",
            "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DLSHk4DqlSG_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b9ac6e0f-4443-4fb2-f1c5-bc0ad6f630e1"
      },
      "cell_type": "code",
      "source": [
        "# imports needed and set up logging\n",
        "import gensim \n",
        "import nltk\n",
        "import logging\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "W7clWk6elSHD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset: Game of Thrones books\n",
        "\n",
        "The most important thing is data. We'll use five volumes of Game of Thrones I downloaded from\n",
        "https://github.com/nihitx/game-of-thrones- and applied a basic cleaning to speed up the process.\n",
        "\n",
        "Download the file to use today here: http://www.lsi.upc.edu/~cristinae/labNLP1/got.5books.clean.txt\n",
        "\n",
        "Do you see part of the extension? _.clean._ That's the MOST IMPORTANT thing in machine learning: take care of your data, look at it, clean it, look at it again. We'll do some cleaning and pre-processing in the next lab.\n",
        "\n",
        "Now, let's take a closer look at this data below by printing the first line. "
      ]
    },
    {
      "metadata": {
        "id": "lzwoAeQAlSHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b0c55bd1-567a-4633-a7b9-5376424c62e8"
      },
      "cell_type": "code",
      "source": [
        "# Open the file and print the first line\n",
        "dataFile=\"got.5books.clean.txt\"\n",
        "\n",
        "## CODE HERE\n",
        "with open (dataFile, 'rb') as f:\n",
        "    for i,line in enumerate (f):\n",
        "        print(line)\n",
        "        break\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"We should start back,\" Gared urged as the woods began to grow dark around them. \"The wildlings are dead.\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nBggAD-WlSHH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Read files into a list\n",
        "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. We'll do first a mild pre-processing using `gensim.utils.simple_preprocess(sentence)`. This does some basic pre-processing (lowercase tokens, ignoring tokens that are too short or too long) and returns a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zmx6lxW8lSHH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e3cc0608-8912-4e4c-f0b0-b46cb47cea56"
      },
      "cell_type": "code",
      "source": [
        "# Write a function `readInput(inputFile)` that reads a file and applies the `simple_preprocess`\n",
        "def readInput(inputFile):\n",
        "    \"\"\"Method to read the input file\"\"\"\n",
        "    ## CODE HERE\n",
        "    logging.info(\"reading file {0}.. this may take a while\".format(inputFile))\n",
        "    with open (inputFile, 'rb') as f:\n",
        "        for i,line in enumerate (f):\n",
        "            if (i%10000==0):\n",
        "                logging.info(\"read {0} lines\".format(i))\n",
        "            yield gensim.utils.simple_preprocess(line)\n",
        "\n",
        "\n",
        "# read the tokenized file into a list (sentences) of lists (tokens) named `sentences`\n",
        "## CODE HERE\n",
        "sentences = list(readInput(dataFile))\n",
        "logging.info(\"Done reading data file\")\n",
        "# print some examples\n",
        "## CODE HERE\n",
        "print(sentences[0])\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-23 19:07:12,632 : INFO : reading file got.5books.clean.txt.. this may take a while\n",
            "2019-04-23 19:07:12,634 : INFO : read 0 lines\n",
            "2019-04-23 19:07:13,343 : INFO : read 10000 lines\n",
            "2019-04-23 19:07:14,014 : INFO : read 20000 lines\n",
            "2019-04-23 19:07:14,421 : INFO : Done reading data file\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[u'we', u'should', u'start', u'back', u'gared', u'urged', u'as', u'the', u'woods', u'began', u'to', u'grow', u'dark', u'around', u'them', u'the', u'wildlings', u'are', u'dead']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qq54Ao9nlSHK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the Word2Vec model\n",
        "\n",
        "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the data, a list (sentences) of lists (tokens) for a complete corpus. Word2Vec uses all these tokens to internally create a vocabulary\n",
        "\n",
        "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Remember you are training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn. "
      ]
    },
    {
      "metadata": {
        "id": "ErxWb3N0lSHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        },
        "outputId": "d889995a-418c-4df8-cae2-6b10253799aa"
      },
      "cell_type": "code",
      "source": [
        "# Define a basic Word2Vec model (gensim.models.Word2Vec) with CBOW and train (model.train) it on `sentences`\n",
        "\n",
        "## CODE HERE\n",
        "model = gensim.models.Word2Vec (sentences, size=100, window=3, min_count=2, workers=1)\n",
        "model.train(sentences, total_examples=len(sentences), epochs=10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-23 19:11:41,148 : INFO : collecting all words and their counts\n",
            "2019-04-23 19:11:41,150 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-04-23 19:11:41,245 : INFO : PROGRESS: at sentence #10000, processed 405638 words, keeping 13375 word types\n",
            "2019-04-23 19:11:41,335 : INFO : PROGRESS: at sentence #20000, processed 784788 words, keeping 17230 word types\n",
            "2019-04-23 19:11:41,390 : INFO : collected 18740 word types from a corpus of 1010744 raw words and 25690 sentences\n",
            "2019-04-23 19:11:41,392 : INFO : Loading a fresh vocabulary\n",
            "2019-04-23 19:11:41,433 : INFO : effective_min_count=2 retains 13673 unique words (72% of original 18740, drops 5067)\n",
            "2019-04-23 19:11:41,434 : INFO : effective_min_count=2 leaves 1005677 word corpus (99% of original 1010744, drops 5067)\n",
            "2019-04-23 19:11:41,478 : INFO : deleting the raw counts dictionary of 18740 items\n",
            "2019-04-23 19:11:41,480 : INFO : sample=0.001 downsamples 49 most-common words\n",
            "2019-04-23 19:11:41,481 : INFO : downsampling leaves estimated 766311 word corpus (76.2% of prior 1005677)\n",
            "2019-04-23 19:11:41,537 : INFO : estimated required memory for 13673 words and 100 dimensions: 17774900 bytes\n",
            "2019-04-23 19:11:41,538 : INFO : resetting layer weights\n",
            "2019-04-23 19:11:41,671 : INFO : training model with 1 workers on 13673 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
            "2019-04-23 19:11:42,684 : INFO : EPOCH 1 - PROGRESS: at 55.09% examples, 426484 words/s, in_qsize 2, out_qsize 0\n",
            "2019-04-23 19:11:43,445 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:43,447 : INFO : EPOCH - 1 : training on 1010744 raw words (766017 effective words) took 1.8s, 432805 effective words/s\n",
            "2019-04-23 19:11:44,469 : INFO : EPOCH 2 - PROGRESS: at 58.14% examples, 446211 words/s, in_qsize 2, out_qsize 0\n",
            "2019-04-23 19:11:45,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:45,190 : INFO : EPOCH - 2 : training on 1010744 raw words (766445 effective words) took 1.7s, 441968 effective words/s\n",
            "2019-04-23 19:11:46,204 : INFO : EPOCH 3 - PROGRESS: at 56.84% examples, 441560 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:11:46,933 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:46,935 : INFO : EPOCH - 3 : training on 1010744 raw words (766266 effective words) took 1.7s, 441002 effective words/s\n",
            "2019-04-23 19:11:47,962 : INFO : EPOCH 4 - PROGRESS: at 58.14% examples, 445568 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:11:48,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:48,671 : INFO : EPOCH - 4 : training on 1010744 raw words (766007 effective words) took 1.7s, 444438 effective words/s\n",
            "2019-04-23 19:11:49,692 : INFO : EPOCH 5 - PROGRESS: at 56.03% examples, 433327 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:11:50,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:50,433 : INFO : EPOCH - 5 : training on 1010744 raw words (766358 effective words) took 1.7s, 438211 effective words/s\n",
            "2019-04-23 19:11:50,438 : INFO : training on a 5053720 raw words (3831093 effective words) took 8.8s, 437061 effective words/s\n",
            "2019-04-23 19:11:50,447 : WARNING : Effective 'alpha' higher than previous training cycles\n",
            "2019-04-23 19:11:50,448 : INFO : training model with 1 workers on 13673 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
            "2019-04-23 19:11:51,472 : INFO : EPOCH 1 - PROGRESS: at 56.84% examples, 437657 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:11:52,208 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:52,210 : INFO : EPOCH - 1 : training on 1010744 raw words (766107 effective words) took 1.8s, 437138 effective words/s\n",
            "2019-04-23 19:11:53,226 : INFO : EPOCH 2 - PROGRESS: at 56.03% examples, 433072 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:11:53,983 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:53,985 : INFO : EPOCH - 2 : training on 1010744 raw words (766346 effective words) took 1.8s, 433553 effective words/s\n",
            "2019-04-23 19:11:55,002 : INFO : EPOCH 3 - PROGRESS: at 56.84% examples, 442340 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:11:55,732 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:55,733 : INFO : EPOCH - 3 : training on 1010744 raw words (766379 effective words) took 1.7s, 441272 effective words/s\n",
            "2019-04-23 19:11:56,742 : INFO : EPOCH 4 - PROGRESS: at 56.84% examples, 443890 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:11:57,451 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:57,452 : INFO : EPOCH - 4 : training on 1010744 raw words (766522 effective words) took 1.7s, 447778 effective words/s\n",
            "2019-04-23 19:11:58,476 : INFO : EPOCH 5 - PROGRESS: at 58.14% examples, 445008 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:11:59,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:11:59,190 : INFO : EPOCH - 5 : training on 1010744 raw words (766473 effective words) took 1.7s, 442923 effective words/s\n",
            "2019-04-23 19:12:00,198 : INFO : EPOCH 6 - PROGRESS: at 56.03% examples, 435314 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:12:00,946 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:12:00,947 : INFO : EPOCH - 6 : training on 1010744 raw words (766802 effective words) took 1.8s, 437245 effective words/s\n",
            "2019-04-23 19:12:01,956 : INFO : EPOCH 7 - PROGRESS: at 56.03% examples, 436616 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:12:02,703 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:12:02,704 : INFO : EPOCH - 7 : training on 1010744 raw words (766781 effective words) took 1.8s, 438131 effective words/s\n",
            "2019-04-23 19:12:03,729 : INFO : EPOCH 8 - PROGRESS: at 60.54% examples, 452083 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:12:04,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:12:04,420 : INFO : EPOCH - 8 : training on 1010744 raw words (765965 effective words) took 1.7s, 448465 effective words/s\n",
            "2019-04-23 19:12:05,445 : INFO : EPOCH 9 - PROGRESS: at 56.84% examples, 436877 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:12:06,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:12:06,189 : INFO : EPOCH - 9 : training on 1010744 raw words (765893 effective words) took 1.8s, 434905 effective words/s\n",
            "2019-04-23 19:12:07,203 : INFO : EPOCH 10 - PROGRESS: at 56.03% examples, 434606 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:12:07,950 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:12:07,951 : INFO : EPOCH - 10 : training on 1010744 raw words (766903 effective words) took 1.8s, 436963 effective words/s\n",
            "2019-04-23 19:12:07,957 : INFO : training on a 10107440 raw words (7664171 effective words) took 17.5s, 437819 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7664171, 10107440)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "d6lWm9jImz4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "c298de33-1249-4362-aa1a-415ec8728bbe"
      },
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec (sentences, size=100, window=3, min_count=2, workers=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-23 19:10:31,834 : INFO : collecting all words and their counts\n",
            "2019-04-23 19:10:31,836 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-04-23 19:10:31,941 : INFO : PROGRESS: at sentence #10000, processed 405638 words, keeping 13375 word types\n",
            "2019-04-23 19:10:32,032 : INFO : PROGRESS: at sentence #20000, processed 784788 words, keeping 17230 word types\n",
            "2019-04-23 19:10:32,090 : INFO : collected 18740 word types from a corpus of 1010744 raw words and 25690 sentences\n",
            "2019-04-23 19:10:32,091 : INFO : Loading a fresh vocabulary\n",
            "2019-04-23 19:10:32,138 : INFO : effective_min_count=2 retains 13673 unique words (72% of original 18740, drops 5067)\n",
            "2019-04-23 19:10:32,140 : INFO : effective_min_count=2 leaves 1005677 word corpus (99% of original 1010744, drops 5067)\n",
            "2019-04-23 19:10:32,184 : INFO : deleting the raw counts dictionary of 18740 items\n",
            "2019-04-23 19:10:32,188 : INFO : sample=0.001 downsamples 49 most-common words\n",
            "2019-04-23 19:10:32,190 : INFO : downsampling leaves estimated 766311 word corpus (76.2% of prior 1005677)\n",
            "2019-04-23 19:10:32,264 : INFO : estimated required memory for 13673 words and 100 dimensions: 17774900 bytes\n",
            "2019-04-23 19:10:32,265 : INFO : resetting layer weights\n",
            "2019-04-23 19:10:32,411 : INFO : training model with 1 workers on 13673 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=3\n",
            "2019-04-23 19:10:33,427 : INFO : EPOCH 1 - PROGRESS: at 56.03% examples, 434947 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:10:34,177 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:10:34,178 : INFO : EPOCH - 1 : training on 1010744 raw words (766017 effective words) took 1.8s, 436246 effective words/s\n",
            "2019-04-23 19:10:35,200 : INFO : EPOCH 2 - PROGRESS: at 56.84% examples, 440818 words/s, in_qsize 2, out_qsize 0\n",
            "2019-04-23 19:10:35,940 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:10:35,942 : INFO : EPOCH - 2 : training on 1010744 raw words (766445 effective words) took 1.7s, 438191 effective words/s\n",
            "2019-04-23 19:10:36,957 : INFO : EPOCH 3 - PROGRESS: at 56.03% examples, 433944 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:10:37,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:10:37,711 : INFO : EPOCH - 3 : training on 1010744 raw words (766266 effective words) took 1.8s, 435168 effective words/s\n",
            "2019-04-23 19:10:38,722 : INFO : EPOCH 4 - PROGRESS: at 55.09% examples, 428669 words/s, in_qsize 2, out_qsize 0\n",
            "2019-04-23 19:10:39,494 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:10:39,496 : INFO : EPOCH - 4 : training on 1010744 raw words (766007 effective words) took 1.8s, 431531 effective words/s\n",
            "2019-04-23 19:10:40,514 : INFO : EPOCH 5 - PROGRESS: at 58.14% examples, 447789 words/s, in_qsize 1, out_qsize 0\n",
            "2019-04-23 19:10:41,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-04-23 19:10:41,231 : INFO : EPOCH - 5 : training on 1010744 raw words (766358 effective words) took 1.7s, 443948 effective words/s\n",
            "2019-04-23 19:10:41,235 : INFO : training on a 5053720 raw words (3831093 effective words) took 8.8s, 434253 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "7H3OQLTTlSHL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Understanding the parameters:\n",
        "\n",
        "```\n",
        "model = gensim.models.Word2Vec (documents, size=100, window=10, min_count=2, workers=2, sg=0)\n",
        "```\n",
        "\n",
        "#### `size`\n",
        "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. \n",
        "\n",
        "#### `window`\n",
        "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
        "\n",
        "#### `min_count`\n",
        "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
        "\n",
        "#### `workers`\n",
        "How many threads to use\n",
        "\n",
        "#### `sg`\n",
        "Training algorithm: CBOW (0) or skip-gram (1)\n"
      ]
    },
    {
      "metadata": {
        "id": "8kwQCWkMlSHQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ClLkhHollSHS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### What have you trained?\n",
        "Several functions allow to explore the results. The `most_similar` function returns the top 10 similar words to a given input word. `similarity` returns the similarity between two words that are present in the vocabulary. `doesnt_match` returns the most dissimilar word with a list of words. Let's play with these functions.\n"
      ]
    },
    {
      "metadata": {
        "id": "bkjyPS1blSHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d91f71ea-7793-4df3-f5c1-8c7139a0cdc1"
      },
      "cell_type": "code",
      "source": [
        "# Chose a word to see the 10 closest words with `most_similar`\n",
        "w1 = \"throne\"\n",
        "## CODE HERE\n",
        "model.most_similar(positive=w1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(u'islands', 0.6903221607208252),\n",
              " (u'fleet', 0.6046836376190186),\n",
              " (u'holt', 0.6003888845443726),\n",
              " (u'chair', 0.5949640274047852),\n",
              " (u'council', 0.590110182762146),\n",
              " (u'realm', 0.5506998896598816),\n",
              " (u'kingslayer', 0.5349704027175903),\n",
              " (u'spikes', 0.5182492733001709),\n",
              " (u'tourney', 0.5094053149223328),\n",
              " (u'price', 0.5049958229064941)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "ZM6N_oRSlSHW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What happens if the word is not in the vocabulary? We are using a tiny corpus in a specific domain..."
      ]
    },
    {
      "metadata": {
        "id": "QG_P1OVslSHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "23aa5c2a-2446-4080-8d22-9cc6f2d9ac47"
      },
      "cell_type": "code",
      "source": [
        "# Chose a word that you think does not belong to the corpus to see the 10 closest words\n",
        "## CODE HERE\n",
        "w1 = \"omelette\"\n",
        "model.most_similar(positive=w1, topn=6)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5cdc98a7cf05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"omelette\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/base_any2vec.pyc\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'omelette' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "rA2mUN3glSHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can also specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related with `most_similar(positive=w1,negative=w2,topn=n)`. "
      ]
    },
    {
      "metadata": {
        "id": "3lJIgVl9lSHa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "de27eafe-4ad5-4310-9713-56c0b74c3d10"
      },
      "cell_type": "code",
      "source": [
        "# get everything related to stuff on the bed for instance\n",
        "## CODE HERE\n",
        "w1 = [\"bed\",\"sheet\",\"pillow\"]\n",
        "w2 = [\"face\"]\n",
        "model.wv.most_similar(positive=w1, negative=w2, topn=10)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(u'cart', 0.7314779758453369),\n",
              " (u'basin', 0.7270445823669434),\n",
              " (u'bunk', 0.6982508897781372),\n",
              " (u'tub', 0.6680843830108643),\n",
              " (u'bedrobe', 0.6667254567146301),\n",
              " (u'horsehide', 0.664029598236084),\n",
              " (u'pillows', 0.6630938649177551),\n",
              " (u'cushion', 0.6628068089485168),\n",
              " (u'cask', 0.6619012355804443),\n",
              " (u'cracks', 0.661659836769104)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "TCamVIHKlSHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# try what happens without the negative constraint\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YSbpJ31-lSHf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Calculate some similarities now"
      ]
    },
    {
      "metadata": {
        "id": "HYVeYoz2lSHg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d35cf06-615b-43b6-cc00-92dd612dfce2"
      },
      "cell_type": "code",
      "source": [
        "# similarity between two different words\n",
        "## CODE HERE\n",
        "w1 = \"bed\"\n",
        "w2 = \"sleep\"\n",
        "model.wv.similarity(w1, w2)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6102705"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "gtZ9YfyMlSHj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# similarity between two identical words\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0bazYaw3lSHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34734afd-eba3-4db9-e98e-9c5839b14731"
      },
      "cell_type": "code",
      "source": [
        "# similarity between two unrelated words\n",
        "## CODE HERE\n",
        "w1 = \"happy\"\n",
        "w2 = \"sad\"\n",
        "model.wv.similarity(w1, w2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.61103094"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "_yY4R2IelSHo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Under the hood, the above three snippets compute the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
      ]
    },
    {
      "metadata": {
        "id": "56u_ujm6lSHp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Find the odd one out\n",
        "You can use Word2Vec to find odd items given a list of items with `doesnt_match`."
      ]
    },
    {
      "metadata": {
        "id": "9YOjUwvplSHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a090a114-55d4-4220-eaef-56cb51fc3aa1"
      },
      "cell_type": "code",
      "source": [
        "# Define a list of words and look for the strange word\n",
        "# Which one is the odd one out in this list?\n",
        "## CODE HERE\n",
        "model.wv.doesnt_match([\"snow\",\"winter\",\"sword\"])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sword'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "RgVIL6odlSHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Which one is the odd one out in this other list?\n",
        "## CODE HERE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hps6LO03lSHx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pre-trained vectors"
      ]
    },
    {
      "metadata": {
        "id": "D8PsL80VlSHy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll use now better vectors that have been trained using large corpora such as Wikipedia and Gigaword. `gensim` also allows to load pre-trained embeddings so that now we can do the same we have done with our word2vec embeddings but using general Glove vectors. "
      ]
    },
    {
      "metadata": {
        "id": "i9kWNwcllSH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "489f8d4c-12f4-44b4-fb95-6d4da3d85169"
      },
      "cell_type": "code",
      "source": [
        "#Download and load the model\n",
        "import gensim.downloader as api\n",
        "model_gigaword = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# If you have them already just load them\n",
        "# from gensim.models import KeyedVectors\n",
        "# gloveModel=\"PATH_TO_FILE\"\n",
        "# model_gigaword = KeyedVectors.load_word2vec_format(gloveModel, binary=False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-23 19:21:45,791 : INFO : Creating /root/gensim-data\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-04-23 19:22:07,920 : INFO : glove-wiki-gigaword-100 downloaded\n",
            "2019-04-23 19:22:07,922 : INFO : loading projection weights from /root/gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.gz\n",
            "2019-04-23 19:22:07,923 : WARNING : this function is deprecated, use smart_open.open instead\n",
            "2019-04-23 19:23:05,875 : INFO : loaded (400000, 100) matrix from /root/gensim-data/glove-wiki-gigaword-100/glove-wiki-gigaword-100.gz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QllDgD1elSH4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "28fd2561-16e1-4e45-afa4-a57d651c5731"
      },
      "cell_type": "code",
      "source": [
        "# find the similarity between two words. \n",
        "# Use the same examples as before an also some examples with out-of-domain vocabulary. I'm sure the word \"phone\" was\n",
        "# not in the vocabulary before!\n",
        "\n",
        "## CODE HERE\n",
        "model_gigaword.wv.most_similar(positive=[\"phone\",\"winter\"], topn=10)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(u'summer', 0.7735719680786133),\n",
              " (u'day', 0.7169382572174072),\n",
              " (u'days', 0.7116138339042664),\n",
              " (u'spring', 0.7099618315696716),\n",
              " (u'time', 0.7047103643417358),\n",
              " (u'next', 0.6884782910346985),\n",
              " (u'weekend', 0.6871427297592163),\n",
              " (u'week', 0.6843944787979126),\n",
              " (u'telephone', 0.6821471452713013),\n",
              " (u'even', 0.6820520162582397)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "futs2qRDlSIA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# as before, get everything related to stuff on the bed\n",
        "\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZLUYxVH-lSIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The famous (king - man) + woman\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1uzin-ClSIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# What happened with the small GoT model?\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZAdYJdmAlSIJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data processing\n"
      ]
    },
    {
      "metadata": {
        "id": "cwlvYEqzlSIK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see what does stemming, part-of-speech tagging and lemmatisation to our corpus.\n",
        "\n",
        "### Stemming with Porter Stemmer"
      ]
    },
    {
      "metadata": {
        "id": "Kmq2q6bplSIK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PorterStemmer uses **suffix stripping** to produce stems. The algorithm does not follow linguistics rather a set of rules for different cases that are applied in phases (step by step) to generate stems. Therefore PorterStemmer does not often generate stems that are actual English words. It uses the rules to decide whether it is wise to strip a suffix.\n",
        "\n",
        "So why using it? Is simple, fast and reduces sparsity! "
      ]
    },
    {
      "metadata": {
        "id": "qnmkqYDRlSIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# initialise the stemmer\n",
        "porterStemmer=PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "luwTxntElSIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "043ca18b-a34b-497b-b028-3c667fc6a398"
      },
      "cell_type": "code",
      "source": [
        "# Let's see the first sentence before and after stemming to understand what we are doing.\n",
        "# porterStemmer.stem(word) does the stemming\n",
        "## CODE HERE\n",
        "\n",
        "stemSentences=[]\n",
        "print(sentences[0])\n",
        "for word in sentences[0]:\n",
        "  stemSentences.append(porterStemmer.stem(word))\n",
        "print(stemSentences)\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[u'we', u'should', u'start', u'back', u'gared', u'urged', u'as', u'the', u'woods', u'began', u'to', u'grow', u'dark', u'around', u'them', u'the', u'wildlings', u'are', u'dead']\n",
            "[u'we', u'should', u'start', u'back', u'gare', u'urg', u'as', u'the', u'wood', u'began', u'to', u'grow', u'dark', u'around', u'them', u'the', u'wildl', u'are', u'dead']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T6D9X8hXlSIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "44dd1631-7017-473c-960a-1c116c46a6a4"
      },
      "cell_type": "code",
      "source": [
        "# It's OK, let's do the whole corpus\n",
        "# Stem the full corpus\n",
        "## CODE HERE\n",
        "\n",
        "# Look some examples\n",
        "example=16169\n",
        "print(sentences[example])\n",
        "print(stemmedSentences[example])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[u'she', u'will', u'without', u'highgarden', u'the', u'lannisters', u'have', u'no', u'hope', u'of', u'keeping', u'joffrey', u'on', u'his', u'throne', u'if', u'my', u'son', u'the', u'lord', u'oaf', u'asks', u'she', u'will', u'have', u'no', u'choice', u'but', u'to', u'grant', u'his', u'request']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a9123f6a483a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16169\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmedSentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'stemmedSentences' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6isEl_M1lSIV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word embeddings on the stemmed corpus"
      ]
    },
    {
      "metadata": {
        "id": "2zzXZhJslSIW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Another training with word2vec, now with stems `modelStems`\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRErA_Z8lSIZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Explore similarities in `modelStems`"
      ]
    },
    {
      "metadata": {
        "id": "-ZD6YpYvlSIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w1 = \"throne\"\n",
        "## CODE HERE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wccIlxo7lSIc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# More examples?\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OyyGXvw4lSIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lemmatisation with a WordNet lemmatiser\n",
        "\n",
        "Lemmatization, unlike stemming, reduces the inflected words properly ensuring that the root word, _lemma_ belongs to the language.\n",
        "NLTK uses a WordNet Lemmatiser that uses the WordNet Database to lookup lemmas of words."
      ]
    },
    {
      "metadata": {
        "id": "k_djQJv2lSIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "26d889f0-3385-406f-fae3-abc4911d1887"
      },
      "cell_type": "code",
      "source": [
        "# Import the package and initialise the lemmatiser\n",
        "# We need to download Wordnet too\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialise the lemmatiser\n",
        "lemmatiser = WordNetLemmatizer()\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LuA1bDrelSIi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "39a4f109-e973-47d1-ca5a-c33a00959f1b"
      },
      "cell_type": "code",
      "source": [
        "# Let's see the first sentence before and after lemmatising\n",
        "## CODE HERE\n",
        "lemSentence=[]\n",
        "print(sentences[0])\n",
        "for word in sentences[0]:\n",
        "  lemSentence.append(lemmatiser.lemmatize(word))\n",
        "print(lemSentence)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[u'we', u'should', u'start', u'back', u'gared', u'urged', u'as', u'the', u'woods', u'began', u'to', u'grow', u'dark', u'around', u'them', u'the', u'wildlings', u'are', u'dead']\n",
            "[u'we', u'should', u'start', u'back', u'gared', u'urged', u'a', u'the', u'wood', u'began', u'to', u'grow', u'dark', u'around', u'them', u'the', u'wildlings', u'are', u'dead']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fofIwWzAlSIl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Are you happy with that? The `lemmatize(word)` function also allow to include information about the PoS of the word `lemmatize(word, PoS)`. Let's us it!"
      ]
    },
    {
      "metadata": {
        "id": "NPweROTJlSIm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports needed for PoS tagging\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# You might need to download this\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Write a function to map PoS tag in wordnet to the first letter only\n",
        "def getWordnetPoS(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    # WordNet POS tags are only: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r', ADJ_SAT = 'a'\n",
        "\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tagDict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tagDict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Tag with PoS the first sentence of the corpus and print it\n",
        "## CODE HERE\n",
        "\n",
        "# Lemmatise the full corpus with the information of PoS now\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q4LOHz6ylSIo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# It's OK, let's lemmatise the whole corpus\n",
        "## CODE HERE\n",
        "\n",
        "# Look again at some examples\n",
        "print(sentences[example])\n",
        "print(lemSentences[example])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JO0s-T8clSIq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word embeddings on the lemmatised corpus"
      ]
    },
    {
      "metadata": {
        "id": "uuyDmNielSIr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Another training with word2vec, now with lemmas, create the model `model Lemmas`\n",
        "## CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O0wLPARflSIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Explore again similarities, distances, leave one out..."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "xxlmZ-JklSIt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w1 = \"throne\"\n",
        "## CODE HERE\n",
        "\n",
        "## ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OjUAI96AlSIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualisation"
      ]
    },
    {
      "metadata": {
        "id": "TEHk-no2lSIw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we will visualise the _n_-dimensional word embeddings by projecting them down to 2-dimensional x,y coordinate pairs. \n",
        "Several techniques exist (PCA, t-SNE, etc). We use PCA in the following (PCA class in `sklearn.decomposition`)"
      ]
    },
    {
      "metadata": {
        "id": "J3YVPG-plSIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports needed for the visualisation\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "# fit a 2d PCA model to the vectors\n",
        "## CODE HERE\n",
        "\n",
        "# create a scatter plot of the projection\n",
        "## CODE HERE\n",
        "\n",
        "# add the labels to the plot\n",
        "## CODE HERE\n",
        "\n",
        "# look at the plot\n",
        "pyplot.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RreJgcZJlSIy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Too much information. Let's select only a subset of words"
      ]
    },
    {
      "metadata": {
        "id": "E64z0wFzlSIy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select what we wanna see ('most_similar' words to something for instance)\n",
        "setToPlot = modelLemmas.wv.most_similar(positive='throne', topn=10)\n",
        "\n",
        "# Look for the vectors for the desired words only, and store them as vectorX and vectorY\n",
        "vectorX =  []\n",
        "vectorY =  []\n",
        "words = []\n",
        "## CODE HERE\n",
        "\n",
        "# create the scatter plot for these words\n",
        "## CODE HERE\n",
        "\n",
        "# add the labels\n",
        "## CODE HERE\n",
        "\n",
        "# look at the plot\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E3B_VCa8lSI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can do many more things, but the best way to learn is parctice by yourself. "
      ]
    },
    {
      "metadata": {
        "id": "76XXDYijlSI2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What do I do now?\n",
        "\n",
        "All the exercise has been done with almost a _toy corpus_ (only 25k sentences, 1M words, 19k types). Everything is fast in this case but the quality is limited. I recommend that you work with a real corpus now.\n",
        "\n",
        "* Download a corpus of tweets, of news or a Wikipedia edition.\n",
        "* Clean and preprocess the data. No stemming or lemmatisation is usually applied with large corpora.\n",
        "* Train word vectors with different software: word2vec, glove, fasttext\n",
        "* Load them and explore the embeddings, you can still use `gensim`\n",
        "* Evaluate the embeddings. The functions `evaluate_word_pairs` and `evaluate_word_analogies` can help on that, you only need to download standard test sets to use a gold standard.\n"
      ]
    },
    {
      "metadata": {
        "id": "IvIVLEDulSI4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}